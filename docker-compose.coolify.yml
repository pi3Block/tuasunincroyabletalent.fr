# ===========================================
# Docker Compose for Coolify - Production Ready
# ===========================================
# Single self-contained file for Coolify deployment
# GPU NVIDIA required for worker service
#
# SHARED INFRASTRUCTURE (2026-02-26):
# - PostgreSQL: shared-postgres (postgis/postgis:16-3.4-alpine + pgvector)
#   Database: voicejury_db, User: augmenter (shared superuser)
# - Redis: shared-redis (redis:7-alpine)
#   DB index 2 (broker + sessions)
# - LiteLLM: litellm-proxy:4000 (LLM gateway, Groq free tier)
# - Langfuse: langfuse:3000 (LLM tracing)
# - Flower: flower-voicejury.augmenter.pro (Celery monitoring, Redis DB 2)
# - All accessed via coolify network DNS
#
# BUILD OPTIMIZATION:
# - .dockerignore in each service dir (reduces context by ~200MB)
# - Worker uses gpu-worker-base shared image (~2min vs ~15min)
# - BuildKit cache mounts for npm/pip
#
# FRONTEND: Migrated to Next.js on Hostinger (https://kiaraoke.fr)
# Repo: https://github.com/pi3Block/frontend.kiaraoke.fr

services:
  # ============================================
  # Backend API - FastAPI + Uvicorn
  # ============================================
  api:
    build:
      context: ./backend
      dockerfile: Dockerfile
    expose:
      - "8080"
    environment:
      # Shared infrastructure — hostnames are Coolify container names
      # DATABASE_URL and REDIS_URL must be set in Coolify env panel
      # Use PgBouncer :6432 for connection pooling (PgBouncer 1.21+ supports prepared statements)
      # Example: postgresql://augmenter:${PG_PASSWORD}@pgbouncer:6432/voicejury_db
      - DATABASE_URL=${DATABASE_URL}
      - REDIS_URL=${REDIS_URL}
      - OLLAMA_HOST=${OLLAMA_HOST:-http://host.docker.internal:11435}
      - SPOTIFY_CLIENT_ID=${SPOTIFY_CLIENT_ID}
      - SPOTIFY_CLIENT_SECRET=${SPOTIFY_CLIENT_SECRET}
      - GENIUS_API_TOKEN=${GENIUS_API_TOKEN:-}
      - SECRET_KEY=${SECRET_KEY}
      - DEBUG=false
      # Remote storage — storages.augmenter.pro (replaces local audio-data volume)
      - STORAGE_URL=${STORAGE_URL:-https://storages.augmenter.pro}
      - STORAGE_API_KEY=${STORAGE_API_KEY:-}
      - STORAGE_BUCKET=${STORAGE_BUCKET:-kiaraoke}
      - AUDIO_TEMP_DIR=/tmp/kiaraoke
      - AUDIO_OUTPUT_DIR=/tmp/kiaraoke
      # Sentry error tracking (optional)
      - SENTRY_DSN=${SENTRY_DSN:-}
      - SENTRY_ENVIRONMENT=${SENTRY_ENVIRONMENT:-production}
    extra_hosts:
      - "host.docker.internal:host-gateway"
    deploy:
      resources:
        limits:
          memory: 512M
    restart: unless-stopped
    networks:
      - voicejury-network
      - coolify
    labels:
      - traefik.enable=true
      # API accessible on both domains
      - traefik.http.routers.voicejury-api.rule=Host(`api.tuasunincroyabletalent.fr`) || Host(`api.kiaraoke.fr`)
      - traefik.http.routers.voicejury-api.entrypoints=https
      - traefik.http.routers.voicejury-api.tls.certresolver=letsencrypt
      - traefik.http.services.voicejury-api.loadbalancer.server.port=8080
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # ============================================
  # Worker Heavy - Multi-GPU (Demucs + CREPE)
  # ============================================
  # Dedicated GPUs — no Ollama sharing, no time-sharing, no OOM:
  #   cuda:0 = GPU 1 (RTX 3080, 10 GB) → Demucs + de-bleeding
  #   cuda:1 = GPU 2 (RTX 3070, 8 GB) → CREPE pitch extraction
  # Whisper: shared-whisper HTTP (GPU 3, separate container)
  # Jury LLM: 100% LiteLLM/Groq (no local GPU needed)
  #
  # Memory budget:
  #   cuda:0 (GPU 1): Demucs ~5.5 GB peak + de-bleeding ~0.5 GB = ~6 GB / 10 GB
  #   cuda:1 (GPU 2): CREPE full ~1 GB / 8 GB
  worker-heavy:
    build:
      context: ./worker
      dockerfile: Dockerfile.optimized
    command: ["celery", "-A", "tasks.celery_app", "worker", "--loglevel=info", "--pool=solo", "-Q", "gpu-heavy,gpu,default", "-n", "heavy@%h", "-B"]
    volumes:
      - model-cache:/root/.cache
    environment:
      - REDIS_URL=${REDIS_URL}
      - DATABASE_URL=${DATABASE_URL}
      # Whisper: shared-whisper HTTP (GPU 3) → Groq cloud fallback
      - SHARED_WHISPER_URL=${SHARED_WHISPER_URL:-http://shared-whisper:9000}
      - WHISPER_MODEL=${WHISPER_MODEL:-turbo}
      - WHISPER_LOCAL_FALLBACK=false
      - GROQ_API_KEY=${GROQ_API_KEY:-}
      # Multi-GPU device assignment
      - DEMUCS_DEVICE=cuda:0
      - CREPE_DEVICE=cuda:1
      # Remote storage — storages.augmenter.pro
      - STORAGE_URL=${STORAGE_URL:-https://storages.augmenter.pro}
      - STORAGE_API_KEY=${STORAGE_API_KEY:-}
      - STORAGE_BUCKET=${STORAGE_BUCKET:-kiaraoke}
      - AUDIO_TEMP_DIR=/tmp/kiaraoke
      - AUDIO_OUTPUT_DIR=/tmp/kiaraoke
      # Langfuse tracing
      - LANGFUSE_PUBLIC_KEY=${LANGFUSE_PUBLIC_KEY}
      - LANGFUSE_SECRET_KEY=${LANGFUSE_SECRET_KEY}
      - LANGFUSE_BASE_URL=${LANGFUSE_BASE_URL:-http://langfuse-wogs8c0wgg8o4gckkg4oc4o8:3000}
      # LiteLLM proxy — jury 100% cloud (Groq qwen3-32b → qwen3-8b fallback)
      - LITELLM_HOST=${LITELLM_HOST:-http://host.docker.internal:4000}
      - LITELLM_API_KEY=${LITELLM_API_KEY:-}
      - LITELLM_JURY_MODEL=${LITELLM_JURY_MODEL:-jury-comment}
      - LITELLM_JURY_FALLBACK_MODEL=${LITELLM_JURY_FALLBACK_MODEL:-jury-comment-fallback}
      # Sentry error tracking (optional)
      - SENTRY_DSN=${SENTRY_DSN:-}
      - SENTRY_ENVIRONMENT=${SENTRY_ENVIRONMENT:-production}
    extra_hosts:
      - "host.docker.internal:host-gateway"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['GPU-bdb1f5e4-d03c-b5c1-bb9c-59f3d08d161d', 'GPU-c99d136d-ff10-5223-3919-9a28e55ffc60']
              capabilities: [gpu]
        limits:
          memory: 8G
    restart: unless-stopped
    networks:
      - voicejury-network
      - coolify

# ============================================
# Volumes (Persistent Data)
# ============================================
# audio-data volume removed: audio storage migrated to storages.augmenter.pro
# GPU tasks use /tmp/kiaraoke/ for processing (auto-cleaned after each task)
volumes:
  model-cache:

# ============================================
# Networks
# ============================================
networks:
  voicejury-network:
    driver: bridge
  coolify:
    external: true
