# ===========================================
# Docker Compose for Coolify - Production Ready
# ===========================================
# Single self-contained file for Coolify deployment
# GPU NVIDIA required for worker service
#
# SHARED INFRASTRUCTURE (2026-02-26):
# - PostgreSQL: shared-postgres (postgis/postgis:16-3.4-alpine + pgvector)
#   Database: voicejury_db, User: augmenter (shared superuser)
# - Redis: shared-redis (redis:7-alpine)
#   DB index 2 (broker + sessions)
# - LiteLLM: litellm-proxy:4000 (LLM gateway, Groq free tier)
# - Langfuse: langfuse:3000 (LLM tracing)
# - Flower: flower-voicejury.augmenter.pro (Celery monitoring, Redis DB 2)
# - All accessed via coolify network DNS
#
# BUILD OPTIMIZATION:
# - .dockerignore in each service dir (reduces context by ~200MB)
# - Worker uses gpu-worker-base shared image (~2min vs ~15min)
# - BuildKit cache mounts for npm/pip
#
# FRONTEND: Migrated to Next.js on Hostinger (https://kiaraoke.fr)
# Repo: https://github.com/pi3Block/frontend.kiaraoke.fr

services:
  # ============================================
  # Backend API - FastAPI + Uvicorn
  # ============================================
  api:
    build:
      context: ./backend
      dockerfile: Dockerfile
    expose:
      - "8080"
    volumes:
      - audio-data:/app/audio_files
    environment:
      # Shared infrastructure — hostnames are Coolify container names
      # DATABASE_URL and REDIS_URL must be set in Coolify env panel
      # Use PgBouncer :6432 for connection pooling (PgBouncer 1.21+ supports prepared statements)
      # Example: postgresql://augmenter:${PG_PASSWORD}@pgbouncer:6432/voicejury_db
      - DATABASE_URL=${DATABASE_URL}
      - REDIS_URL=${REDIS_URL}
      - OLLAMA_HOST=${OLLAMA_HOST:-http://host.docker.internal:11435}
      - SPOTIFY_CLIENT_ID=${SPOTIFY_CLIENT_ID}
      - SPOTIFY_CLIENT_SECRET=${SPOTIFY_CLIENT_SECRET}
      - GENIUS_API_TOKEN=${GENIUS_API_TOKEN:-}
      - SECRET_KEY=${SECRET_KEY}
      - DEBUG=false
      # Sentry error tracking (optional)
      - SENTRY_DSN=${SENTRY_DSN:-}
      - SENTRY_ENVIRONMENT=${SENTRY_ENVIRONMENT:-production}
    extra_hosts:
      - "host.docker.internal:host-gateway"
    deploy:
      resources:
        limits:
          memory: 512M
    restart: unless-stopped
    networks:
      - voicejury-network
      - coolify
    labels:
      - traefik.enable=true
      # API accessible on both domains
      - traefik.http.routers.voicejury-api.rule=Host(`api.tuasunincroyabletalent.fr`) || Host(`api.kiaraoke.fr`)
      - traefik.http.routers.voicejury-api.entrypoints=https
      - traefik.http.routers.voicejury-api.tls.certresolver=letsencrypt
      - traefik.http.services.voicejury-api.loadbalancer.server.port=8080
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # ============================================
  # Worker Heavy - GPU (Demucs, CREPE)
  # ============================================
  # GPU 0 (RTX 3070, 8 Go, GPU-85c38fae) — SHARED with Ollama Light via time-sharing
  # Pipeline.py unloads Ollama model (keep_alive:0) before Demucs loads.
  # LLM jury: LiteLLM proxy → Groq qwen3-32b (free) → Ollama fallback
  # Whisper: shared-whisper HTTP → Groq Whisper fallback (free)
  worker-heavy:
    build:
      context: ./worker
      dockerfile: Dockerfile.optimized
    command: ["celery", "-A", "tasks.celery_app", "worker", "--loglevel=info", "--pool=solo", "-Q", "gpu-heavy,gpu,default", "-n", "heavy@%h", "-B"]
    volumes:
      - audio-data:/app/audio_files
      - model-cache:/root/.cache
    environment:
      # Shared infrastructure — Coolify container names with UUID suffix
      # DATABASE_URL and REDIS_URL must be set in Coolify env panel
      - REDIS_URL=${REDIS_URL}
      - DATABASE_URL=${DATABASE_URL}
      - OLLAMA_HOST=${OLLAMA_HOST:-http://host.docker.internal:11435}
      - SHARED_WHISPER_URL=${SHARED_WHISPER_URL:-http://shared-whisper:9000}
      - WHISPER_MODEL=${WHISPER_MODEL:-turbo}
      # Whisper: shared-whisper primary, Groq cloud fallback, local disabled
      - WHISPER_LOCAL_FALLBACK=false
      - GROQ_API_KEY=${GROQ_API_KEY:-}
      - CUDA_VISIBLE_DEVICES=0
      # Langfuse tracing
      - LANGFUSE_PUBLIC_KEY=${LANGFUSE_PUBLIC_KEY}
      - LANGFUSE_SECRET_KEY=${LANGFUSE_SECRET_KEY}
      - LANGFUSE_BASE_URL=${LANGFUSE_BASE_URL:-http://langfuse-wogs8c0wgg8o4gckkg4oc4o8:3000}
      # LiteLLM proxy — jury generation via Groq qwen3-32b (free)
      - LITELLM_HOST=${LITELLM_HOST:-http://host.docker.internal:4000}
      - LITELLM_API_KEY=${LITELLM_API_KEY:-}
      - LITELLM_JURY_MODEL=${LITELLM_JURY_MODEL:-jury-comment}
      # Sentry error tracking (optional)
      - SENTRY_DSN=${SENTRY_DSN:-}
      - SENTRY_ENVIRONMENT=${SENTRY_ENVIRONMENT:-production}
    extra_hosts:
      - "host.docker.internal:host-gateway"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['GPU-85c38fae-1bcf-d15b-4f36-e1f581acd76e']
              capabilities: [gpu]
        limits:
          memory: 6G
    restart: unless-stopped
    networks:
      - voicejury-network
      - coolify

  # ============================================
  # Worker Pool - GPU secondaire (optionnel)
  # ============================================
  # Second GPU worker for light tasks (CREPE pitch analysis)
  # Activate with COMPOSE_PROFILES=multi-gpu
  # For single GPU setup: don't set COMPOSE_PROFILES
  worker-pool:
    build:
      context: ./worker
      dockerfile: Dockerfile.optimized
    command: ["celery", "-A", "tasks.celery_app", "worker", "--loglevel=info", "--pool=solo", "-Q", "gpu", "-n", "pool@%h"]
    volumes:
      - audio-data:/app/audio_files
      - model-cache:/root/.cache
    environment:
      # DATABASE_URL and REDIS_URL must be set in Coolify env panel
      - REDIS_URL=${REDIS_URL}
      - DATABASE_URL=${DATABASE_URL}
      - OLLAMA_HOST=${OLLAMA_HOST:-http://host.docker.internal:11435}
      - SHARED_WHISPER_URL=${SHARED_WHISPER_URL:-http://shared-whisper:9000}
      - WHISPER_MODEL=${WHISPER_MODEL:-turbo}
      # Whisper: shared-whisper primary, Groq cloud fallback, local disabled
      - WHISPER_LOCAL_FALLBACK=false
      - GROQ_API_KEY=${GROQ_API_KEY:-}
    extra_hosts:
      - "host.docker.internal:host-gateway"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['GPU-85c38fae-1bcf-d15b-4f36-e1f581acd76e']
              capabilities: [gpu]
        limits:
          memory: 512M
    profiles:
      - multi-gpu
    restart: unless-stopped
    networks:
      - voicejury-network
      - coolify

# ============================================
# Volumes (Persistent Data)
# ============================================
volumes:
  audio-data:
  model-cache:

# ============================================
# Networks
# ============================================
networks:
  voicejury-network:
    driver: bridge
  coolify:
    external: true
